#####################
# Performance Impacting Settings
#####################
fp16 = True

# model specific impact
return_timestamps = False

#####################
# CHANGEABLE SETTINGS
#####################

# Output Directory Name
output_tag = large_v3

# Cluster Specific Settings: These Must Match The Requested Resources In The .BS File
num_workers = 1
cpus_per_trial = 0
gpus_per_trial = 1
use_gpu = True
eval_batch_size = 1
prefetch_batches = 0

# For Reproducibility: Import To Use The Same Random Seed For Evaluation
random_seed = 1337

# resume training
resume_evaluation = False


#####################
# Model type and debugging
#####################
model_type = openai/whisper-large-v3
debug = False
dataset_name = eg_dataset_complete_v3
run_on_local_machine = True
h5 = True
peft = True
model_ckpt_path = /Users/chrvt/Documents/GitHub/asr-finetune/finetune/output/peft_model/TorchTrainer_a581ef30_3_alpha=4,learning_rate=0.0017,per_device_train_batch_size=8,rank=9,warmup_steps=0,weight_decay=0.0291_2025-04-18_17-36-10/checkpoint_000023/checkpoint/adapter_model
path_to_data = /Users/chrvt/Documents/GitHub/asr-finetune/data_example/datasets/eg_dataset_complete_v3_sharded/test
dataset_name = eg_dataset_complete_v3
run_on_local_machine = False
