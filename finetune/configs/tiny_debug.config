#####################
# Performance Impacting Settings
#####################

num_train_epochs = 8
max_warmup_steps = 0
max_concurrent_trials = 20
per_device_train_batch_size = 16

fp16 = True
num_samples = 20

# Trainer/Scheduler Specific Settings
search_schedule_mode = large_small_OPTUNA
reduction_factor = 4
metric_to_optimize = eval_wer

#####################
# CHANGEABLE SETTINGS
#####################

# Output Directory Name
output_tag = whisper_tiny_debug_OPTUNA

# Logging Specific Settings
save_steps = 4
eval_steps = 2
logging_steps = 1

# Cluster Specific Settings: These Must Match The Requested Resources In The .BS File
num_workers = 1
cpus_per_trial = 1
gpus_per_trial = 0

# For Reproducibility: Import To Use The Same Random Seed For Evaluation
random_seed = 1337

reuse_actors = True
resume_training = True


#####################
# DO NOT CHANGE THESE SETTINGS UNLESS YOU KNOW WHAT YOU ARE DOING
#####################
generation_max_length = 225
model_type = openai/whisper-tiny
debug = True
path_to_data = ../data/bug
num_to_keep = 1
hyperparameters=learning_rate,weight_decay
run_on_local_machine = False
