#####################
# Performance Impacting Settings
#####################
# Training time impacting
num_train_epochs = 8
per_device_train_batch_size = 16
fp16 = True
num_samples = 2

# model specific impact
return_timestamps = False

# Trainer/Scheduler Specific Settings
search_schedule_mode = large_small_OPTUNA #currently available: large_small_BOHB and large_small_OPTUNA
reduction_factor = 4
metric_to_optimize = eval_loss #currently available: eval_wer and eval_loss

# List of hyperparameters to finetune
hyperparameters=learning_rate,weight_decay #currently available: learning_rate,weight_decay and warmup_steps
max_warmup_steps = 0


#####################
# CHANGEABLE SETTINGS
#####################

# Output Directory Name
output_tag = whisper_large_jan

# Logging Specific Settings
save_steps = 30
eval_steps = 15
logging_steps = 15

# Cluster Specific Settings: These Must Match The Requested Resources In The .BS File
num_workers = 1
cpus_per_trial = 2
gpus_per_trial = 1
use_gpu = True
dataloader_num_workers = 1

# For Reproducibility: Import To Use The Same Random Seed For Evaluation
random_seed = 1337
test_split = 0.2

# Scalability and storage
max_concurrent_trials = 20
reuse_actors = True
num_to_keep = 1

# resume training
resume_training = False


#####################
# Model type and debugging
#####################
model_type = openai/whisper-large-v3
debug = True
path_to_data = /scratch/usr/bemchrvt/data/
dataset_name = eg_dataset_subset_1000.h5
run_on_local_machine = False
h5 = True
peft = False

