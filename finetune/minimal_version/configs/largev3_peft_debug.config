#####################
# Performance Impacting Settings
#####################
# Training time impacting
num_train_epochs = 1
per_device_train_batch_size = 8
per_device_eval_batch_size = 8
fp16 = True
num_samples = 4

# model specific impact
return_timestamps = False

# Trainer/Scheduler Specific Settings
search_schedule_mode = large_small_OPTUNA #currently available: large_small_BOHB and large_small_OPTUNA
reduction_factor = 4
metric_to_optimize = eval_loss #,eval_loss #currently available: eval_wer and eval_loss
modes = min #,min #which modes corresponding to the metric to optimise
eval_sample_fraction = 0.001 # splitting eval set into smaller sizes for faster training
max_steps = 100 # max number of steps

# Logging Specific Settings
save_steps = 2
eval_steps = 1
logging_steps = 1


# List of hyperparameters to finetune
hyperparameters=learning_rate,batch_size,rank,alpha #currently available: learning_rate,weight_decay, scheduler, warmup_steps, rank, alpha (peft only)
max_warmup_steps = 0


#####################
# CHANGEABLE SETTINGS
#####################

# Output Directory Name
output_tag = v3_large_debug


# Cluster Specific Settings: These Must Match The Requested Resources In The .BS File
num_workers = 1
cpus_per_trial = 2
gpus_per_trial = 0.5
use_gpu = True
prefetch_batches = 1

# For Reproducibility: Import To Use The Same Random Seed For Evaluation
random_seed = 1337

# Scalability and storage
max_concurrent_trials = 4
reuse_actors = True
num_to_keep = 1

# resume training
resume_training = False

#####################
# Model type and debugging
#####################
model_type = openai/whisper-large-v3
debug = True # uses less data then provided
path_to_data =/scratch/usr/bemchrvt/data/
dataset_name = eg_dataset_complete_v3_sharded
run_on_local_machine = False
peft = True

load_ds_in_trainer = True
data_mode = parquet
