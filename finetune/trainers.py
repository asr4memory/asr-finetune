"""Collection of different trainers and corresponding training Arguments.
"""

from utils import  steps_per_epoch, normalize
import evaluate

# laod transformers
from transformers import Seq2SeqTrainingArguments
from transformers import Seq2SeqTrainer


# For hyperparameter optimization
import ray
import ray.train
from ray.train.huggingface.transformers import prepare_trainer, RayTrainReportCallback

# get models
from models import get_whisper_models as get_models


# only those hyperparameter which should be optimized
def make_seq2seq_training_kwargs(args):
    """Training Arguments Filter for the train_model function.

    This is not stricly required as we can also pass args into the train_model. However, it serves as an overview of the
    relevant training arguments for the Seq2Seq Trainer:
    https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Seq2SeqTrainingArguments

    In addition to the provided args, we set some default values based on the original Whisper Hyperparameters:
    https://cdn.openai.com/papers/whisper.pdf, in particular the beta1 and beta 2 values of the AdamW optimizer (which
    is the default optimzier) differ to the default parameters.

    Args:
        args (dict): dictionary of keyboard arguments
    Returns:
       training_kwargs (dict): dictionary of relevant training arguments
    """
    training_kwargs = {
        "output_dir": args.output_dir,
        "gradient_accumulation_steps": args.gradient_accumulation_steps,
        "len_train_set": args.len_train_set,
        "num_train_epochs": args.num_train_epochs,
        "max_steps": 0,
        "generation_max_length": args.generation_max_length,
        "save_steps": args.save_steps,
        "eval_steps": args.eval_steps,
        "logging_steps": args.logging_steps,
        "eval_delay": args.eval_delay, #int(args.max_steps/10),
        "adam_beta1": 0.9,
        "adam_beta2": 0.98,
        "seed": args.random_seed,
        "fp16": args.fp16,
        "model_type": args.model_type,
        "target_language": args.target_language,
        "return_timestamps": args.return_timestamps,
        # "torch_empty_cache_steps": 1, # This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about 10% slower performance.
    }
    return training_kwargs
def train_whisper_model(config, training_kwargs=None, data_collator=None):
    """Main training function for one specific Hyper Parameter configuration.

    Each ray-worker (each hyperparameter configuration) executes this function. The training data needs to be in a ray
    training iter object which requires a data_collator. However, since we already collated the data in the pre-
    processing (see DataCollatorSpeechSeq2SeqWithPadding), we simply use the identity as collator.

    The reporting and logging is done by ray tune automatically. To adopt this function for another fine-tuning project,
    follow similar steps:
    1. define the required models
    2. define the evaluation metric
    3. load the data into ray tune iter
    4. define trainer Instance (here: Seq2SeqTrainer)
    5. End with:
        trainer.add_callback(RayTrainReportCallback())
        trainer = prepare_trainer(trainer)
        trainer.train()

    Requires:
       get_models (function): A function loading the necessary models for training and evaluation
       compute_metrics (function): A function which computes the metrics (WER in our case)

    Args:
       config (tune.TuneConfig): Config File with Hyperparameter instances (automaticall generated by ray)
       training_kwargs (dict): Dictionary of training arguments for the Hugging Face Seq2SeqTrainer
    """
    # get models
    model, feature_extractor, tokenizer, processor = get_models(training_kwargs["model_type"],training_kwargs["target_language"],return_timestamps=training_kwargs["return_timestamps"])
    del training_kwargs["model_type"]
    del training_kwargs["target_language"]
    del training_kwargs["return_timestamps"]

    # Define metric for evaluation
    metric = evaluate.load("wer")
    def compute_metrics(pred):
        """Performance Metric calculator, here: Word Error Rate (WER)

        Note: 'Normalizes' the strings before calculating the WER.

        Requires:
            Initialized Tokenizer for decoded the predicitions and labels into human language
            WER metric from the evaluate package
        Args:
            pred (dict): a dictionary with keys "predictions" and "label_ids"
        Returns:
            (dict): A dictionary with key "wer" and the corresponding value
        """
        pred_ids = pred.predictions
        label_ids = pred.label_ids

        # replace -100 with the pad_token_id
        label_ids[label_ids == -100] = tokenizer.pad_token_id

        # we do not want to group tokens when computing the metrics
        pred_str = normalize(tokenizer.batch_decode(pred_ids, skip_special_tokens=True))
        label_str = normalize(tokenizer.batch_decode(label_ids, skip_special_tokens=True))
        wer = 100 * metric.compute(predictions=pred_str, references=label_str)

        return {"wer": wer}

    train_ds = ray.train.get_dataset_shard("train")
    eval_ds = ray.train.get_dataset_shard("eval")

    # this is a hack - as train_ds from Ray requires the data_collotor, so does Seq2SeqTrainer from HF
    # but collating twice does not make sense, therefore we introduce the indentity collator
    def data_collator_id(input):
        """Identity Collator"""
        return input

    # the data collator takes our pre-processed data and prepares PyTorch tensors ready for the model.
    train_ds_iterable = train_ds.iter_torch_batches(
        batch_size=config["per_device_train_batch_size"], collate_fn=data_collator)

    eval_ds_iterable = eval_ds.iter_torch_batches(batch_size=config["per_device_train_batch_size"], collate_fn=data_collator)

    training_kwargs["max_steps"] = steps_per_epoch(training_kwargs["len_train_set"],config["per_device_train_batch_size"]) * training_kwargs["num_train_epochs"]

    del training_kwargs['len_train_set']  # we remove this as its not part of Seq2Seq
    del training_kwargs['num_train_epochs']

    training_args = Seq2SeqTrainingArguments(
        gradient_checkpointing=True,
        evaluation_strategy="steps",
        save_strategy = "steps",
        predict_with_generate=True,
        report_to=["tensorboard"],
        load_best_model_at_end=True,
        metric_for_best_model="wer",
        greater_is_better=False,
        push_to_hub=False,
        do_eval=True,
        **config,
        **training_kwargs,
    )

    trainer = Seq2SeqTrainer(
        args=training_args,
        model=model,
        train_dataset=train_ds_iterable,
        eval_dataset=eval_ds_iterable,
        data_collator=data_collator_id,
        compute_metrics=compute_metrics,
        # tokenizer=tokenizer,  we don't need this as we do the pre-processing before
    )

    trainer.add_callback(RayTrainReportCallback())
    trainer = prepare_trainer(trainer)
    trainer.train()

    # processor.save_pretrained(training_args.output_dir) # TODO: is this really necessary?

